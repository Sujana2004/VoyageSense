{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sujana2004/VoyageSense/blob/main/VoyageSenseModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4Hkhg9CEvi_G",
        "outputId": "5616695f-8966-4a19-a755-5c89fb1c7386"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸš€ Starting Ollama with OpenAI Compatibility Mode...\n",
            "============================================================\n",
            "\n",
            "ğŸ“¦ Installing Ollama...\n",
            "âœ… Ollama installed!\n",
            "\n",
            "ğŸ”§ Starting Ollama server...\n",
            "ğŸ” Verifying Ollama...\n",
            "âœ… Ollama is running!\n",
            "NAME    ID    SIZE    MODIFIED \n",
            "\n",
            "ğŸ“¥ Downloading model (2-5 minutes)...\n",
            "âœ… Mistral model ready!\n",
            "\n",
            "ğŸ§ª Testing OpenAI compatibility...\n",
            "âš ï¸ OpenAI endpoint not found. Testing Ollama native endpoint...\n",
            "âŒ API test failed: istral:7b-instruct-q4_0' not found\"}\n",
            "HTTP_CODE:404\n",
            "\n",
            "ğŸŒ Setting up CloudFlare tunnel...\n",
            "âœ… CloudFlared ready!\n",
            "\n",
            "ğŸ”— Creating public tunnel...\n",
            "============================================================\n",
            "\n",
            "ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰\n",
            "SUCCESS! Ollama is publicly accessible!\n",
            "ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰\n",
            "\n",
            "ğŸ“‹ COPY THIS URL (no spaces):\n",
            "======================================================================\n",
            "https://walnut-races-campaign-indexes.trycloudflare.com\n",
            "======================================================================\n",
            "\n",
            "ğŸ§ª Testing public tunnel...\n",
            "âœ… Tunnel response code: 000\n",
            "âš ï¸ OpenAI endpoint test result: \n",
            "HTTP_CODE:000\n",
            "\n",
            "ğŸ“ Your application.properties configuration:\n",
            "----------------------------------------------------------------------\n",
            "# âš ï¸ If OpenAI compatibility doesn't work, try:\n",
            "# Option 1: Use Ollama-specific configuration\n",
            "spring.ai.ollama.base-url=https://walnut-races-campaign-indexes.trycloudflare.com\n",
            "spring.ai.ollama.chat.options.model=mistral:7b-instruct-q4_0\n",
            "\n",
            "# Option 2: Keep OpenAI config but check if model exists\n",
            "spring.ai.openai.base-url=https://walnut-races-campaign-indexes.trycloudflare.com\n",
            "spring.ai.openai.api-key=ollama\n",
            "spring.ai.openai.chat.options.model=mistral:7b-instruct-q4_0\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "ğŸ“¦ Available models:\n",
            "Could not list models\n",
            "\n",
            "============================================================\n",
            "âš ï¸ IMPORTANT NOTES:\n",
            "1. KEEP THIS NOTEBOOK RUNNING\n",
            "2. If you get 404, ensure model name matches exactly\n",
            "3. Try 'mistral:7b' instead of 'mistral:7b-instruct-q4_0' if needed\n",
            "============================================================\n",
            "\n",
            "ğŸ’š Started at 04:29:24 - Keep tab open!\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-437509500.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# # ============================================\n",
        "# # FIXED: OLLAMA WITH OPENAI COMPATIBILITY\n",
        "# # ============================================\n",
        "\n",
        "# import subprocess\n",
        "# import threading\n",
        "# import time\n",
        "# import re\n",
        "# import os\n",
        "\n",
        "# print(\"ğŸš€ Starting Ollama with OpenAI Compatibility Mode...\")\n",
        "# print(\"=\"*60)\n",
        "\n",
        "# # 1. Install Ollama\n",
        "# print(\"\\nğŸ“¦ Installing Ollama...\")\n",
        "# !curl -fsSL https://ollama.com/install.sh | sh > /dev/null 2>&1\n",
        "# print(\"âœ… Ollama installed!\")\n",
        "\n",
        "# # 2. Start Ollama with OpenAI compatibility\n",
        "# print(\"\\nğŸ”§ Starting Ollama server...\")\n",
        "\n",
        "# # Set environment for OpenAI compatibility\n",
        "# os.environ['OLLAMA_ORIGINS'] = '*'\n",
        "# os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
        "\n",
        "# def run_ollama():\n",
        "#     env = os.environ.copy()\n",
        "#     env.update({\n",
        "#         'OLLAMA_ORIGINS': '*',\n",
        "#         'OLLAMA_HOST': '0.0.0.0:11434'\n",
        "#     })\n",
        "#     subprocess.run(['ollama', 'serve'], env=env,\n",
        "#                    stdout=subprocess.DEVNULL,\n",
        "#                    stderr=subprocess.DEVNULL)\n",
        "\n",
        "# ollama_thread = threading.Thread(target=run_ollama, daemon=True)\n",
        "# ollama_thread.start()\n",
        "# time.sleep(7)  # Give more time to start\n",
        "\n",
        "# # 3. Verify Ollama is running\n",
        "# print(\"ğŸ” Verifying Ollama...\")\n",
        "# result = subprocess.run(['curl', '-s', 'http://localhost:11434'],\n",
        "#                        capture_output=True, text=True)\n",
        "# if \"Ollama\" in result.stdout or result.returncode == 0:\n",
        "#     print(\"âœ… Ollama is running!\")\n",
        "# else:\n",
        "#     print(\"âš ï¸ Ollama might still be starting...\")\n",
        "\n",
        "# !ollama list\n",
        "\n",
        "# # 4. Pull model\n",
        "# print(\"\\nğŸ“¥ Downloading model (2-5 minutes)...\")\n",
        "# model_result = subprocess.run(['ollama', 'pull', 'llama3.1:8b-instruct-q4_0'], #mistral:7b-instruct-q4_0\n",
        "#                              capture_output=True, text=True)\n",
        "# if model_result.returncode == 0:\n",
        "#     print(\"âœ… Mistral model ready!\")\n",
        "# else:\n",
        "#     print(\"âš ï¸ Trying alternative model...\")\n",
        "#     !ollama pull tinyllama\n",
        "#     print(\"âœ… TinyLlama model ready!\")\n",
        "\n",
        "# # 5. Test OpenAI compatibility endpoint\n",
        "# print(\"\\nğŸ§ª Testing OpenAI compatibility...\")\n",
        "\n",
        "# # Test if Ollama responds to OpenAI-style requests\n",
        "# test_payload = '''\n",
        "# {\n",
        "#   \"model\": \"mistral:7b-instruct-q4_0\",\n",
        "#   \"messages\": [{\"role\": \"user\", \"content\": \"Hi\"}],\n",
        "#   \"stream\": false\n",
        "# }\n",
        "# '''\n",
        "\n",
        "# # Test the OpenAI endpoint\n",
        "# test_result = subprocess.run([\n",
        "#     'curl', '-X', 'POST',\n",
        "#     'http://localhost:11434/v1/chat/completions',\n",
        "#     '-H', 'Content-Type: application/json',\n",
        "#     '-d', test_payload,\n",
        "#     '-s', '-w', '\\nHTTP_CODE:%{http_code}'\n",
        "# ], capture_output=True, text=True)\n",
        "\n",
        "# if \"HTTP_CODE:200\" in test_result.stdout:\n",
        "#     print(\"âœ… OpenAI compatibility mode is working!\")\n",
        "# elif \"HTTP_CODE:404\" in test_result.stdout:\n",
        "#     print(\"âš ï¸ OpenAI endpoint not found. Testing Ollama native endpoint...\")\n",
        "\n",
        "#     # Test native Ollama endpoint\n",
        "#     native_test = subprocess.run([\n",
        "#         'curl', '-X', 'POST',\n",
        "#         'http://localhost:11434/api/generate',\n",
        "#         '-H', 'Content-Type: application/json',\n",
        "#         '-d', '{\"model\": \"mistral:7b-instruct-q4_0\", \"prompt\": \"Hi\", \"stream\": false}',\n",
        "#         '-s', '-w', '\\nHTTP_CODE:%{http_code}'\n",
        "#     ], capture_output=True, text=True)\n",
        "\n",
        "#     if \"HTTP_CODE:200\" in native_test.stdout:\n",
        "#         print(\"âœ… Native Ollama API is working!\")\n",
        "#         print(\"âš ï¸ Note: You may need to use Ollama-specific Spring AI configuration\")\n",
        "#     else:\n",
        "#         print(f\"âŒ API test failed: {native_test.stdout[-50:]}\")\n",
        "\n",
        "# # 6. Install CloudFlared\n",
        "# print(\"\\nğŸŒ Setting up CloudFlare tunnel...\")\n",
        "# if not os.path.exists('./cloudflared-linux-amd64'):\n",
        "#     !wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
        "#     !chmod +x cloudflared-linux-amd64\n",
        "# print(\"âœ… CloudFlared ready!\")\n",
        "\n",
        "# # 7. Start tunnel\n",
        "# print(\"\\nğŸ”— Creating public tunnel...\")\n",
        "# print(\"=\"*60)\n",
        "\n",
        "# # Start tunnel in background\n",
        "# tunnel_process = subprocess.Popen([\n",
        "#     './cloudflared-linux-amd64', 'tunnel',\n",
        "#     '--url', 'http://localhost:11434'\n",
        "# ], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "\n",
        "# # Capture URL\n",
        "# public_url = None\n",
        "# start_time = time.time()\n",
        "\n",
        "# for line in tunnel_process.stdout:\n",
        "#     # Check for URL\n",
        "#     if 'trycloudflare.com' in line:\n",
        "#         url_match = re.search(r'https://[a-zA-Z0-9\\-]+\\.trycloudflare\\.com', line)\n",
        "#         if url_match and not public_url:\n",
        "#             public_url = url_match.group(0).strip()\n",
        "#             break\n",
        "\n",
        "#     # Timeout after 30 seconds\n",
        "#     if time.time() - start_time > 30:\n",
        "#         break\n",
        "\n",
        "# if public_url:\n",
        "#     print(\"\\n\" + \"ğŸ‰\"*25)\n",
        "#     print(\"SUCCESS! Ollama is publicly accessible!\")\n",
        "#     print(\"ğŸ‰\"*25)\n",
        "\n",
        "#     # Clean URL display\n",
        "#     print(f\"\\nğŸ“‹ COPY THIS URL (no spaces):\")\n",
        "#     print(\"=\"*70)\n",
        "#     print(public_url)\n",
        "#     print(\"=\"*70)\n",
        "\n",
        "#     # Test the public URL\n",
        "#     print(\"\\nğŸ§ª Testing public tunnel...\")\n",
        "\n",
        "#     # Test root endpoint\n",
        "#     public_test = subprocess.run([\n",
        "#         'curl', '-s', '-o', '/dev/null', '-w', '%{http_code}',\n",
        "#         public_url\n",
        "#     ], capture_output=True, text=True, timeout=10)\n",
        "\n",
        "#     print(f\"âœ… Tunnel response code: {public_test.stdout}\")\n",
        "\n",
        "#     # Test OpenAI endpoint through tunnel\n",
        "#     openai_test = subprocess.run([\n",
        "#         'curl', '-X', 'POST',\n",
        "#         f'{public_url}/v1/chat/completions',\n",
        "#         '-H', 'Content-Type: application/json',\n",
        "#         '-d', test_payload,\n",
        "#         '-s', '-w', '\\nHTTP_CODE:%{http_code}'\n",
        "#     ], capture_output=True, text=True, timeout=10)\n",
        "\n",
        "#     if \"HTTP_CODE:200\" in openai_test.stdout:\n",
        "#         print(\"âœ… OpenAI endpoint accessible through tunnel!\")\n",
        "#         config_type = \"OpenAI\"\n",
        "#     elif \"HTTP_CODE:404\" in openai_test.stdout:\n",
        "#         print(\"âš ï¸ OpenAI endpoint returns 404\")\n",
        "#         print(\"â„¹ï¸ This Ollama version might not support OpenAI compatibility\")\n",
        "#         config_type = \"Ollama\"\n",
        "#     else:\n",
        "#         print(f\"âš ï¸ OpenAI endpoint test result: {openai_test.stdout[-50:]}\")\n",
        "#         config_type = \"Unknown\"\n",
        "\n",
        "#     # Show appropriate configuration\n",
        "#     print(\"\\nğŸ“ Your application.properties configuration:\")\n",
        "#     print(\"-\"*70)\n",
        "\n",
        "#     if config_type == \"OpenAI\":\n",
        "#         print(f\"spring.ai.openai.base-url={public_url}\")\n",
        "#         print(\"spring.ai.openai.api-key=ollama\")\n",
        "#         print(\"spring.ai.openai.chat.options.model=mistral:7b-instruct-q4_0\")\n",
        "#         print(\"spring.ai.openai.chat.options.temperature=0.1\")\n",
        "#     else:\n",
        "#         print(\"# âš ï¸ If OpenAI compatibility doesn't work, try:\")\n",
        "#         print(\"# Option 1: Use Ollama-specific configuration\")\n",
        "#         print(f\"spring.ai.ollama.base-url={public_url}\")\n",
        "#         print(\"spring.ai.ollama.chat.options.model=mistral:7b-instruct-q4_0\")\n",
        "#         print(\"\")\n",
        "#         print(\"# Option 2: Keep OpenAI config but check if model exists\")\n",
        "#         print(f\"spring.ai.openai.base-url={public_url}\")\n",
        "#         print(\"spring.ai.openai.api-key=ollama\")\n",
        "#         print(\"spring.ai.openai.chat.options.model=mistral:7b-instruct-q4_0\")\n",
        "\n",
        "#     print(\"-\"*70)\n",
        "\n",
        "#     # List available models\n",
        "#     print(\"\\nğŸ“¦ Available models:\")\n",
        "#     !curl -s {public_url}/api/tags | python -m json.tool 2>/dev/null || echo \"Could not list models\"\n",
        "\n",
        "# else:\n",
        "#     print(\"\\nâŒ Could not capture tunnel URL!\")\n",
        "#     print(\"Checking tunnel output manually...\")\n",
        "#     !ps aux | grep cloudflared\n",
        "#     print(\"\\nTry running this command manually:\")\n",
        "#     print(\"!./cloudflared-linux-amd64 tunnel --url http://localhost:11434\")\n",
        "\n",
        "# print(\"\\n\" + \"=\"*60)\n",
        "# print(\"âš ï¸ IMPORTANT NOTES:\")\n",
        "# print(\"1. KEEP THIS NOTEBOOK RUNNING\")\n",
        "# print(\"2. If you get 404, ensure model name matches exactly\")\n",
        "# print(\"3. Try 'mistral:7b' instead of 'mistral:7b-instruct-q4_0' if needed\")\n",
        "# print(\"=\"*60)\n",
        "\n",
        "# # Keep alive\n",
        "# print(f\"\\nğŸ’š Started at {time.strftime('%H:%M:%S')} - Keep tab open!\")\n",
        "\n",
        "# while True:\n",
        "#     time.sleep(3600)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "W2GAeCP_F5GJ",
        "outputId": "cc5dd7bc-964c-4dcb-f43b-23a39bc53d10"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸš€ Starting Ollama with OpenAI Compatibility Mode...\n",
            "============================================================\n",
            "\n",
            "ğŸ“¦ Installing Ollama...\n",
            "âœ… Ollama installed!\n",
            "\n",
            "ğŸ”§ Starting Ollama server...\n",
            "âœ… Ollama is running!\n",
            "\n",
            "ğŸ“¦ Available models:\n",
            "NAME                         ID              SIZE      MODIFIED      \n",
            "llama3.1:8b-instruct-q4_0    42182419e950    4.7 GB    2 minutes ago    \n",
            "âœ… llama3.1:8b-instruct-q4_0 already available!\n",
            "\n",
            "ğŸ§ª Testing OpenAI compatibility...\n",
            "âœ… OpenAI compatibility working!\n",
            "\n",
            "ğŸŒ Setting up CloudFlare tunnel...\n",
            "âœ… CloudFlared ready!\n",
            "\n",
            "ğŸ”— Starting CloudFlare tunnel (takes 15-30 seconds)...\n",
            "======================================================================\n",
            "\n",
            "ğŸ‰ Tunnel established!\n",
            "\n",
            "â³ Waiting for tunnel to stabilize (10s)...\n",
            "\n",
            "ğŸ§ª Testing tunnel connectivity...\n",
            "  Root endpoint: HTTP 200\n",
            "  /api/tags: HTTP 200\n",
            "  /v1/chat/completions: âœ… Working\n",
            "\n",
            "======================================================================\n",
            "ğŸ“‹ YOUR TUNNEL URL:\n",
            "======================================================================\n",
            "https://relate-susan-pepper-manner.trycloudflare.com\n",
            "======================================================================\n",
            "\n",
            "ğŸ“ COPY THIS TO application.properties:\n",
            "----------------------------------------------------------------------\n",
            "spring.ai.openai.base-url=https://relate-susan-pepper-manner.trycloudflare.com\n",
            "spring.ai.openai.api-key=ollama\n",
            "spring.ai.openai.chat.options.model=llama3.1:8b-instruct-q4_0\n",
            "spring.ai.openai.chat.options.temperature=0.7\n",
            "spring.ai.openai.chat.options.max-tokens=1024\n",
            "spring.ai.openai.chat.options.top-p=0.9\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "ğŸ“¦ Verifying models via tunnel:\n",
            "{\n",
            "    \"models\": [\n",
            "        {\n",
            "            \"name\": \"llama3.1:8b-instruct-q4_0\",\n",
            "            \"model\": \"llama3.1:8b-instruct-q4_0\",\n",
            "            \"modified_at\": \"2025-11-13T04:29:20.183393863Z\",\n",
            "            \"size\": 4661230766,\n",
            "            \"digest\": \"42182419e9508c30c4b1fe55015f06b65f4ca4b9e28a744be55008d21998a093\",\n",
            "            \"details\": {\n",
            "                \"parent_model\": \"\",\n",
            "                \"format\": \"gguf\",\n",
            "                \"family\": \"llama\",\n",
            "                \"families\": [\n",
            "                    \"llama\"\n",
            "                ],\n",
            "                \"parameter_size\": \"8.0B\",\n",
            "                \"quantization_level\": \"Q4_0\"\n",
            "            }\n",
            "        }\n",
            "    ]\n",
            "}\n",
            "\n",
            "======================================================================\n",
            "âš ï¸ IMPORTANT:\n",
            "  1. Keep this notebook tab open\n",
            "  2. Tunnel URL changes each session\n",
            "  3. Session lasts ~12 hours on free Colab\n",
            "======================================================================\n",
            "\n",
            "ğŸ’š Running since 04:32:25\n",
            "ğŸ’¡ Notebook will print status every 30 minutes\n",
            "\n",
            "ğŸ’š Still alive [05:02:25] - 30 min uptime\n",
            "ğŸ’š Still alive [05:32:25] - 60 min uptime\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3434077410.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1800\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 30 minutes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m     \u001b[0mcounter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"ğŸ’š Still alive [{datetime.datetime.now().strftime('%H:%M:%S')}] - {counter*30} min uptime\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# FIXED: OLLAMA WITH PROPER TUNNEL WAIT\n",
        "# ============================================\n",
        "\n",
        "# First Uncomment the previous cell and run and then comment it and run this cell so that model loading will not have issues\n",
        "\n",
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "import re\n",
        "import os\n",
        "\n",
        "print(\"ğŸš€ Starting Ollama with OpenAI Compatibility Mode...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 1. Install Ollama\n",
        "print(\"\\nğŸ“¦ Installing Ollama...\")\n",
        "!curl -fsSL https://ollama.com/install.sh | sh > /dev/null 2>&1\n",
        "print(\"âœ… Ollama installed!\")\n",
        "\n",
        "# 2. Start Ollama\n",
        "print(\"\\nğŸ”§ Starting Ollama server...\")\n",
        "os.environ['OLLAMA_ORIGINS'] = '*'\n",
        "os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
        "\n",
        "def run_ollama():\n",
        "    env = os.environ.copy()\n",
        "    env.update({'OLLAMA_ORIGINS': '*', 'OLLAMA_HOST': '0.0.0.0:11434'})\n",
        "    subprocess.run(['ollama', 'serve'], env=env,\n",
        "                   stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "\n",
        "threading.Thread(target=run_ollama, daemon=True).start()\n",
        "time.sleep(7)\n",
        "\n",
        "# 3. Verify Ollama\n",
        "result = subprocess.run(['curl', '-s', 'http://localhost:11434'],\n",
        "                       capture_output=True, text=True)\n",
        "if \"Ollama\" in result.stdout:\n",
        "    print(\"âœ… Ollama is running!\")\n",
        "else:\n",
        "    print(\"âš ï¸ Ollama might still be starting...\")\n",
        "\n",
        "# 4. Check available models\n",
        "print(\"\\nğŸ“¦ Available models:\")\n",
        "!ollama list\n",
        "\n",
        "# 5. Use llama3.1:8b-instruct (better for travel planning)\n",
        "model_name = \"llama3.1:8b-instruct-q4_0\"\n",
        "\n",
        "# Check if model exists\n",
        "model_check = subprocess.run(['ollama', 'list'], capture_output=True, text=True)\n",
        "if model_name not in model_check.stdout:\n",
        "    print(f\"\\nğŸ“¥ Downloading {model_name} (this takes 5-10 minutes)...\")\n",
        "    !ollama pull llama3.1:8b-instruct\n",
        "    print(\"âœ… Model ready!\")\n",
        "else:\n",
        "    print(f\"âœ… {model_name} already available!\")\n",
        "\n",
        "# 6. Test OpenAI compatibility\n",
        "print(\"\\nğŸ§ª Testing OpenAI compatibility...\")\n",
        "test_payload = f'{{\"model\":\"{model_name}\",\"messages\":[{{\"role\":\"user\",\"content\":\"Hi\"}}],\"stream\":false}}'\n",
        "test_result = subprocess.run([\n",
        "    'curl', '-X', 'POST', 'http://localhost:11434/v1/chat/completions',\n",
        "    '-H', 'Content-Type: application/json',\n",
        "    '-d', test_payload, '-s', '-w', '\\nHTTP:%{http_code}'\n",
        "], capture_output=True, text=True, timeout=30)\n",
        "\n",
        "if \"HTTP:200\" in test_result.stdout:\n",
        "    print(\"âœ… OpenAI compatibility working!\")\n",
        "else:\n",
        "    print(f\"âš ï¸ OpenAI endpoint returned: {test_result.stdout[-50:]}\")\n",
        "\n",
        "# 7. Install CloudFlared\n",
        "print(\"\\nğŸŒ Setting up CloudFlare tunnel...\")\n",
        "if not os.path.exists('./cloudflared-linux-amd64'):\n",
        "    !wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
        "    !chmod +x cloudflared-linux-amd64\n",
        "print(\"âœ… CloudFlared ready!\")\n",
        "\n",
        "# 8. Start tunnel with better capture\n",
        "print(\"\\nğŸ”— Starting CloudFlare tunnel (takes 15-30 seconds)...\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Kill any existing tunnels\n",
        "!pkill -9 cloudflared 2>/dev/null\n",
        "\n",
        "# Start new tunnel and save output\n",
        "tunnel_log = '/tmp/tunnel.log'\n",
        "subprocess.Popen(\n",
        "    ['./cloudflared-linux-amd64', 'tunnel', '--url', 'http://localhost:11434'],\n",
        "    stdout=open(tunnel_log, 'w'),\n",
        "    stderr=subprocess.STDOUT\n",
        ")\n",
        "\n",
        "# Wait for tunnel to establish\n",
        "public_url = None\n",
        "for attempt in range(60):  # Try for 60 seconds\n",
        "    time.sleep(1)\n",
        "\n",
        "    try:\n",
        "        with open(tunnel_log, 'r') as f:\n",
        "            content = f.read()\n",
        "            urls = re.findall(r'https://[a-zA-Z0-9\\-]+\\.trycloudflare\\.com', content)\n",
        "            if urls:\n",
        "                public_url = urls[0].strip()\n",
        "                break\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    if attempt % 5 == 0 and attempt > 0:\n",
        "        print(f\"â³ Waiting for tunnel... {attempt}s\")\n",
        "\n",
        "if not public_url:\n",
        "    print(\"\\nâŒ Could not capture URL. Checking logs...\")\n",
        "    !cat /tmp/tunnel.log\n",
        "    print(\"\\nâš ï¸ Try running this cell again\")\n",
        "else:\n",
        "    print(f\"\\nğŸ‰ Tunnel established!\\n\")\n",
        "\n",
        "    # Wait a bit more for tunnel to stabilize\n",
        "    print(\"â³ Waiting for tunnel to stabilize (10s)...\")\n",
        "    time.sleep(10)\n",
        "\n",
        "    # Test the tunnel\n",
        "    print(\"\\nğŸ§ª Testing tunnel connectivity...\")\n",
        "\n",
        "    # Test 1: Root endpoint\n",
        "    root_test = subprocess.run(\n",
        "        ['curl', '-s', '-m', '10', '-o', '/dev/null', '-w', '%{http_code}', public_url],\n",
        "        capture_output=True, text=True\n",
        "    )\n",
        "    print(f\"  Root endpoint: HTTP {root_test.stdout}\")\n",
        "\n",
        "    # Test 2: API tags\n",
        "    tags_test = subprocess.run(\n",
        "        ['curl', '-s', '-m', '10', '-o', '/dev/null', '-w', '%{http_code}',\n",
        "         f'{public_url}/api/tags'],\n",
        "        capture_output=True, text=True\n",
        "    )\n",
        "    print(f\"  /api/tags: HTTP {tags_test.stdout}\")\n",
        "\n",
        "    # Test 3: OpenAI endpoint\n",
        "    openai_test = subprocess.run([\n",
        "        'curl', '-X', 'POST', '-s', '-m', '30',\n",
        "        f'{public_url}/v1/chat/completions',\n",
        "        '-H', 'Content-Type: application/json',\n",
        "        '-d', test_payload,\n",
        "        '-w', '\\nHTTP:%{http_code}'\n",
        "    ], capture_output=True, text=True)\n",
        "\n",
        "    openai_status = \"200\" in openai_test.stdout or \"HTTP:200\" in openai_test.stdout\n",
        "    print(f\"  /v1/chat/completions: {'âœ… Working' if openai_status else 'âŒ Failed'}\")\n",
        "\n",
        "    # Display configuration\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ğŸ“‹ YOUR TUNNEL URL:\")\n",
        "    print(\"=\"*70)\n",
        "    print(public_url)\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    print(\"\\nğŸ“ COPY THIS TO application.properties:\")\n",
        "    print(\"-\"*70)\n",
        "    print(f\"spring.ai.openai.base-url={public_url}\")\n",
        "    print(\"spring.ai.openai.api-key=ollama\")\n",
        "    print(f\"spring.ai.openai.chat.options.model={model_name}\")\n",
        "    print(\"spring.ai.openai.chat.options.temperature=0.7\")\n",
        "    print(\"spring.ai.openai.chat.options.max-tokens=1024\")\n",
        "    print(\"spring.ai.openai.chat.options.top-p=0.9\")\n",
        "    print(\"-\"*70)\n",
        "\n",
        "    # List models via tunnel\n",
        "    print(\"\\nğŸ“¦ Verifying models via tunnel:\")\n",
        "    !curl -s {public_url}/api/tags | python3 -m json.tool 2>/dev/null || echo \"âš ï¸ Could not list models\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"âš ï¸ IMPORTANT:\")\n",
        "    print(\"  1. Keep this notebook tab open\")\n",
        "    print(\"  2. Tunnel URL changes each session\")\n",
        "    print(\"  3. Session lasts ~12 hours on free Colab\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "# Keep alive with periodic status\n",
        "print(f\"\\nğŸ’š Running since {time.strftime('%H:%M:%S')}\")\n",
        "print(\"ğŸ’¡ Notebook will print status every 30 minutes\\n\")\n",
        "\n",
        "import datetime\n",
        "counter = 0\n",
        "while True:\n",
        "    time.sleep(1800)  # 30 minutes\n",
        "    counter += 1\n",
        "    print(f\"ğŸ’š Still alive [{datetime.datetime.now().strftime('%H:%M:%S')}] - {counter*30} min uptime\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ih4t0jbwwZl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMqp3BC9lWYKEV1WzN0iQF8",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}